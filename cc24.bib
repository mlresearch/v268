@Proceedings{CC-2024,
    booktitle = {Proceedings of The Second AAAI Bridge Program on Continual Causality},
    name = {AAAI Bridge Program on Continual Causality},
    shortname = {CC},
    editor = {Mundt, Martin and Cooper, Keiland W. and Dhami, Devendra Singh and Hayes, Tyler and Herman, Rebecca and Ribeiro, Ad\'ele and Smith, James Seale},
    volume = {268},
    year = {2024},
    start = {2024-02-20},
    end = {2024-02-21},
    published = {2025-07-27},
    conference_url = {https://www.continualcausality.org/},
    address = {Vancouver, Canada}
}

@InProceedings{mian24,
    title = {An Information Theoretic Framework for Continual Learning of Causal Networks.},
    author = {Mian, Osman and Mameche, Sarah},
    pages = {1-10},
    abstract = {Discovering causal networks, especially from observational data alone, is a fundamental yet challenging task. Existing causal discovery algorithms not only rely on strict assumptions such as having i.i.d data, but are also limited to working with static, fully-specified datasets, rendering them incapable of learning causal networks in a continual fashion. In this short paper, we propose an information-theoretic approach that can learn causal networks in a continual fashion, does not require the i.i.d assumption on continually arriving data, and converges to the true underlying causal network as samples within the accumulated batches of data converge to the underlying data generating distribution. Our proposed approach, ConCausD, leverages the Algorithmic Markov Condition to discover causal networks in an online fashion. ConCausD is not only capable of continual learning, it also provides multiple plausible causal graphs at the end of each iteration, while the existing approaches can only predict a single causal network.}
}

@InProceedings{seng24,
    title = {Causality in Flux: Continual Adaptation of Causal Knowledge via Evidence Matching},
    author = {Seng, Jonas and Busch, Florian Peter and Kersting, Kristian},
    pages = {11-20},
    abstract = {Utilising causal knowledge in machine learning (ML) systems yields more robust models with the capability of performing certain extrapolations. However, much of current causality research focuses on deriving causal models in isolation, hence current systems are not capable of updating and improving causal knowledge when new observations arrive. Drawing inspiration from human learning, Continual Learning (CL) aims at updating models given a sequential stream of evidence. Leveraging common patterns and past experiences to gradually improve causal knowledge in ML models is a crucial step towards more robust CL systems. In this work, we propose to learn and update causal models in a lifelong learning setting where causal knowledge explaining newly arriving observations is inferred from similar previously seen observations. We call this framework evidence matching. Further, an analysis of real world data supporting our motivation is provided.}
}